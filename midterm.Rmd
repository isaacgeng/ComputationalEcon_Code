---
title: "Midterm-HW"
author: "Hao GENG"
date: "2019骞<b4>2鏈<88>24鏃<a5>"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r}
#load packages and csv file
library(knitr)
library(ggplot2)
library(dplyr)
library(RColorBrewer)
#library(rworldmap)
#library(ggthemes)
#library(rgdal)
#library(corrplot)
#library(ggfortify)
library(reshape2)
#library(gridExtra)
#library(lubridate)
#library(caret)
#library(psych)
library(geosphere)
library(sp)
#library(doMC)
#registerDoMC(2)
library(lfe)
```

```{r cars}
setwd("C:/Users/耿浩/Documents/Computational_Midterm")
df = read.csv("new.csv",header = TRUE,sep = ",",stringsAsFactors = F,fileEncoding = "GBK")


```
- The floor variable means high/middle/low among the total # of the building
- so we need to split the variable into two.
```{r cars}
df$NFloor <- as.numeric(sapply(df$floor, function(x) strsplit(x,' ')[[1]][2]))
df$vert_loca<- sapply(df$floor, function(x) strsplit(x,' ')[[1]][1])

```
## Drop NAs
```{r cars}
row.has.na <- apply(df, 1, function(x){any(is.na(x))})
sum(row.has.na)
df_nNA <- df[!row.has.na,]
```
## Inspect the nature of variable
```{r cars}
str(df)
# new_summary = apply(df,2,function(x){summary(x) {if (class(x)=="number"|"interger")}})

```
## Including Plots



```{r pressure, echo=FALSE}
df_plot = aggregate(df["price"],list(df$tradeTime),mean)
f = ggplot(data=df_plot,aes(df_plot$Group.1,df_plot$price))
f+geom_col()

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## 4.The regression analysis
### 4.1 Utilization of latitude and longitude data
**Use of the location data**
- Firstly, we use google map to get the main shopping mall center, since shopping malls represents the population flow and the convenience of life.
- Then, convert the data from KML to json and use python to parse it.
- The final data are listed here.
```{r}
business_center = rbind('116.415424,39.909242', '116.459284,39.909456', '116.451473,39.916122', '116.479368,39.909111', '116.483831,39.910658', '116.429801,39.907341', '116.422033,39.915168', '116.374987,39.897448', '116.373432,39.910806', '116.361576,39.917217', '116.357875,39.907564', '116.289296,39.957698', '116.315861,39.978222', '116.454949,39.934388', '116.462932,39.948931', '116.474047,39.949753', '116.436002,39.939768', '116.353326,39.940903', '116.421862,39.89828', '116.504731,40.034154', '116.421132,39.885603', '116.457825,39.875394', '116.449199,39.90921', '116.445487,39.913456', '116.453018,39.934059', '116.535587,39.902494', '116.403408,39.936296', '116.394728,39.93906', '116.340784,39.93753', '116.355472,39.977054', '116.38607,39.895267', '116.395404,39.895835', '116.375116,39.907551', '116.294725,40.095309', '116.398473,39.86067', '116.436281,39.971233', '116.411519,39.913727', '116.186986,39.762466', '116.410253,39.913621', '116.32086,39.970543', '116.295605,39.97327', '116.344357,39.966612', '116.296399,39.908024', '116.366973,39.854444', '116.305175,39.908057', '116.547432,40.066971', '116.373539,39.909456', '116.32343,39.965465', '116.523324,39.940541', '116.490784,39.969983', '116.239235,40.212195', '116.518164,39.923824', '116.405758,39.990849', '116.418074,39.89902', '116.406605,39.994219', '116.458962,39.907251', '116.44892,39.919134', '116.431925,39.940409', '116.227777,39.906592', '116.449714,39.934256', '116.596527,40.009735', '116.338938,39.992361', '116.439242,39.921635', '116.460743,39.89395', '116.488295,39.877337', '116.312658,39.981177', '116.375835,39.909325', '116.285799,39.838628')
business_center =data.frame(business_center) 

```
**Classification into the shopping mall district.**
- firstly, let us get some sense of how far those shopping mall is between each other, so we could decide how much we should set on the distance variable later on we use in classification.
- But this function is not used later.
```{r Classification}
# calculate the distance matrix use the geosphere package
get_geo_distance = function(long1, lat1, long2, lat2, units = "miles") {
  loadNamespace("purrr")
  loadNamespace("geosphere")
  longlat1 = purrr::map2(long1, lat1, function(x,y) c(x,y))
  longlat2 = purrr::map2(long2, lat2, function(x,y) c(x,y))
  distance_list = purrr::map2(longlat1, longlat2, function(x,y) geosphere::distHaversine(x, y))
  distance_m = list_extract(distance_list, position = 1)
  if (units == "km") {
    distance = distance_m / 1000.0;
  }
  else if (units == "miles") {
    distance = distance_m / 1609.344
  }
  else {
    distance = distance_m
    # This will return in meter as same way as distHaversine function. 
  }
  distance
}
```

-Then we use the sq module to calculate the minimum distance and the one shopping center which is closest.The shopping center is indexed by numbers but here I did not retrive it back to the names of them because I did not quite used to use R to do it. I may do it using Python afterwards.
```{r}
bizcenter = colsplit(business_center$business_center, ",", names = c("Lng", "Lat"))
df_nNA_new = df_nNA

coordinates(df_nNA_new) <- c("Lng", "Lat")
coordinates(bizcenter) <- c("Lng", "Lat")
# create the two vectors for loop use
closestSiteVec <- vector(mode = "numeric",length = nrow(df_nNA))
minDistVec     <- vector(mode = "numeric",length = nrow(df_nNA))

for (i in 1 : nrow(df_nNA_new))
  {
    distVec <- spDistsN1(bizcenter,df_nNA_new[i,],longlat = TRUE)
    minDistVec[i] <- min(distVec)
    closestSiteVec[i] <- which.min(distVec)
  }

df_nNA$closest_site = closestSiteVec
rm(closestSiteVec)
df_nNA$closest_dis = minDistVec
rm(minDistVec)
```
### 4.1.2 Consider how to interpret the location information.
- first, we could see which location is the one with the most residents houses.
- second, we could use the group in which they are in to use as a control variable. (FE)
```{r}
g = ggplot(data=df_nNA,aes(df_nNA$closest_site,price))
g + geom_point(aes(colour=factor(df_nNA$bathRoom)))
```

- so this indicates that the price pattern is relative stable across the business cycle.
- and the room type structure across the business center cycle is the same as well.
- So later on we could perform a FE estimation.
- Since this does not asked about the prediction, I use PLM package directly to regress without split it into train set and test set.
--- 
- before that, we need to further convert some variable
```{r}
df_nNA$livingRoom <- as.numeric(df_nNA$livingRoom)
df_nNA$bathRoom <- as.numeric(df_nNA$bathRoom)
df_nNA$drawingRoom <- as.numeric(df_nNA$drawingRoom)
df_nNA$district <- as.factor(df_nNA$district)
#------------
makeBuildingType <- function(x){
  if(!is.na(x)){
    if(x==1){
      return('Tower')
      }
      else if (x==2){
        return('Bungalow')
      }
      else if (x==3){
        return('Mix_plate_tower')
      }
      else if (x==4){
        return('plate')
      }
  else return('wrong_coded')
  }
  else{return('missing')}
}
df_nNA$buildingType <- sapply(df_nNA$buildingType, makeBuildingType)

df_nNA <- data.frame(df_nNA %>% filter(buildingType != 'wrong_coded' & buildingType !='missing'))

makeRenovationCondition <- function(x){
    if(x==1){
        return('Other')
    }
    else if (x==2){
        return('Rough')
    }
    else if (x==3){
        return('Simplicity')
    }
    else if (x==4){
        return('Hardcover')
    }
}
df_nNA$renovationCondition <- sapply(df_nNA$renovationCondition, makeRenovationCondition)

makeBuildingStructure <- function(x){
    if(x==1){
        return('Unknown')
    }
    else if (x==2){
        return('Mix')
    }
    else if (x==3){
        return('Brick_Wood')
    }
    else if (x==4){
        return('Brick_Concrete')
    }
    else if (x==5){
        return('Steel')
    }
    else if (x==6){
        return('Steel_Concrete')
    }
}
df_nNA$buildingStructure <- sapply(df_nNA$buildingStructure, makeBuildingStructure)
df_nNA$elevator <- ifelse(df_nNA$elevator==1,'has_elevator','no_elevator')
df_nNA$constructionTime <-as.numeric(df_nNA$constructionTime)
df_nNA$district <-as.factor(df_nNA$district)
df_nNA$subway <- ifelse(df_nNA$subway==1,'has_subway','no_subway')

df_nNA$fiveYearsProperty <- ifelse(df_nNA$fiveYearsProperty==1,'owner_less_5y','owner_more_5y')

#head(df2)
```
```{r}
# plot of missing data
df_nNA %>% is.na %>% melt %>% 
  ggplot(data = .,aes(x = Var2, y = Var1)) + geom_tile(aes(fill = value,width=0.95)) +
  scale_fill_manual(values = c("grey20","white")) + theme_minimal(14) + 
  theme(axis.text.x  = element_text(angle=45, vjust=.75), 
        legend.position='None',
        legend.direction='horizontal',
        panel.grid.major=element_blank(),
        axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        plot.margin=unit(c(.1,.1,.1,.1), "cm")) + 
  labs(title = 'Missing values in each columns', subtitle='represented as white bars')
df_nNA <- data.frame(df_nNA %>% na.omit())

```
## 4.2 FE regression
### 4.2.1 Performing the reg
**Finally, we use lfe library to do regression, since the matrix is so big i have tried so many ways, finally i found this library, but the formula used in the felm function is kind of redundent, I would appreciate if anyone would indicate a better way to do it.**
```{r}
df_nNA$closest_site = factor(df_nNA$closest_site)
df_nNA$tradeTime = factor(df_nNA$tradeTime)
res = felm(price~	DOM+followers+square+livingRoom+drawingRoom+kitchen+bathRoom+buildingType+constructionTime+	renovationCondition+buildingStructure+ladderRatio+elevator+fiveYearsProperty+subway+district+communityAverage+closest_dis|closest_site+tradeTime, data= df_nNA, exactDOF = 152236)
summary(res)

```
### 4.2.2 interpretation of the regression results 
Here the FE estimates shows that 
- the bigger house is, the harder to sell at high price, i guess it's because people are credit constrained.
- the structure of the rooms (whether having living rooms, bathrooms and drawing rooms) would impact the price.
- construction time shows that the newer the better.
- and most importantly, the closer your house to the closest shopping mall, the higher price you would have. This is consistent with the intuition.

## 5. Further Projects or to dos.
### 5.1 To use structure model (BLP demand estimation and cost of house estimation.)
I would like to incorporate the DOM variable into the structure model, the intuition are listed as follows. 
And since the house is expensive, the probability for you to choose the house the time you saw them is very rare. You usually wouold ponder for a long time for it. But this information reflected by DOM would affect the interpretation of the final model. Imagine a case that there is a house with high quality and reasonable price, but the information about its quality is needed to be verified, thus need time, if you price it even lower than the other similar houses' price, the buyer would doubt more so as to invest more time into the house. If the houses supply is limited, and the DOM is short, then it would imply that this kind of house with the listed price would have more probability to be bought buy agent than its realized market share. So this variable is very informative to be modeled. A model processing information like rational inattention (Matejka and Mckay, 2018) may be helpful.
Beside,the decision of outside good is hard here. Unlike cereals or yogurts or even cars (in US it's relatively cheap), the house in Beijing is so expensive even looking back into 2000's. So if you are one of the followers in the platform, I assume you are very likely to have enough money to enter this market (i.e. you have be able to purchase some house in the market.) So these followers would indicates us some information to infer the potential buyers. Then combining with the total transaction data and the portion of the transaction from Lianjia, we could infer the total market size. The outside choice then could be defined and computed.
And then the following up procedures is standard using BLP. If land transaction data is acquired, the cost function is also estimatable.